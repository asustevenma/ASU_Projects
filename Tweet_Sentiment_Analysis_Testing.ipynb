{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from stemming.porter2 import stem\n",
    "from nltk.classify.api import ClassifierI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Steven Ma/Desktop/ASU - Business Analytics/Applied Project/FYI_Brands_List.csv\", \"r\") as f:\n",
    "    d_reader = csv.DictReader(f)\n",
    "    brand_list = d_reader.fieldnames #get fieldnames from DictReader object and store in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read CSV file and build a dataframe #\n",
    "path = 'C:/Users/Steven Ma/Desktop/ASU - Business Analytics/Applied Project/Tweets'\n",
    "csvFiles = glob.glob(path + \"/*.csv\")\n",
    "Doc = os.listdir(path)\n",
    "\n",
    "file_list = []\n",
    "name_list = []\n",
    "appended_data = []\n",
    "test_df = pd.DataFrame()\n",
    "final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### User-defined Text_Classifier function ###############\n",
    "def Text_Classifier(test_df):\n",
    "    # Manipulating text data #\n",
    "    #time_list = []\n",
    "    influencer_tweets = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        #time_list.append(row['created_at'])\n",
    "        tweet_tuple = list([row['id'][2:-1], row['text'][2:], row['Twitter_ID']])\n",
    "        influencer_tweets.append(tweet_tuple)\n",
    "    \n",
    "    # Text processing #\n",
    "    tweets_test = []\n",
    "    mouse_list = []\n",
    "    hashtag_list = []\n",
    "    sw = stopwords.words(\"english\")\n",
    "    for (tweet_id, words, twitter_id) in influencer_tweets:\n",
    "        new_words = words.split()\n",
    "        for special_word in new_words:\n",
    "            if special_word.startswith('@'):\n",
    "                mouse_list.append(special_word)\n",
    "            elif special_word.startswith('#'):\n",
    "                hashtag_list.append(special_word)\n",
    "        words_removed = [''.join(c for c in s if c not in string.punctuation) for s in new_words]\n",
    "        words_lowered = [e.lower() for e in words_removed]\n",
    "        for single_word in words_lowered:\n",
    "            if single_word.startswith('http'):\n",
    "                words_lowered.remove(single_word)\n",
    "        words_nonstopped = [w for w in words_lowered if w not in sw]\n",
    "        words_stemmed = [stem(txt) for txt in words_nonstopped]\n",
    "        tweets_test.append([tweet_id, words_stemmed, words, twitter_id, mouse_list, hashtag_list, words_nonstopped])\n",
    "    \n",
    "    # Add sentiment in the tuple #\n",
    "    for i in range(len(test_df)):\n",
    "        tweets_test[i] += list([classifier.classify(extract_features(tweets_test[i][1]))])\n",
    "    \n",
    "    # Convert to dataframe #\n",
    "    lable = ['Tweet_ID', 'Tweets', 'Original_Tweet', 'Twitter_ID', 'At-mark', 'Hashtag', 'Tweet_Token', 'Sentiment']\n",
    "    tweet_dataframe = pd.DataFrame(tweets_test, columns = lable)\n",
    "    \n",
    "    return tweet_dataframe\n",
    "############### End of function ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0e1bbeb9bf65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Twitter_ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mclassified_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mText_Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mappended_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassified_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappended_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2a6dd91762a2>\u001b[0m in \u001b[0;36mText_Classifier\u001b[1;34m(test_df)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Add sentiment in the tuple #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtweets_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Convert to dataframe #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "############### Build a dataframe with TweetID, Tokenized Tweets, TwitterID, Sentiment ###############\n",
    "for file_name, file in zip(Doc, csvFiles):\n",
    "    df = pd.read_csv(file, index_col=None, header=0)\n",
    "    df['Twitter_ID'] = file_name[:-4]\n",
    "    classified_df = Text_Classifier(df)\n",
    "    appended_data.append(classified_df)\n",
    "final_df = pd.concat(appended_data)\n",
    "############### End ###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
